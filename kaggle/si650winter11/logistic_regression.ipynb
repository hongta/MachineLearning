{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.20.3\n",
      "3.2.3\n",
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(nltk.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(DATASET_PATH + '/training.txt', header=None, delimiter=\"\\t\", quoting=3)\n",
    "train_data_df.columns = [\"Sentiment\", \"Text\"]\n",
    "\n",
    "test_data_df = pd.read_csv(DATASET_PATH + '/testdata.txt', header=None, delimiter=\"\\t\", quoting=3)\n",
    "test_data_df.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7086, 2), (33052, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.shape, test_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3995\n",
       "0    3091\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caculate the average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.886819079875812"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(s.split(\" \")) for s in train_data_df.Text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    #remove non leters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    #stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=tokenize,\n",
    "                             lowercase=True,\n",
    "                             stop_words=\"english\",\n",
    "                             max_features=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(\n",
    "                                    train_data_df.Text.tolist()+test_data_df.Text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40138, 85)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "corpus_data_features_nd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'amaz', 'angelina', 'awesom', 'beauti', 'becaus', 'boston', 'brokeback', 'citi', 'code', 'cool', 'cruis', 'd', 'da', 'drive', 'francisco', 'friend', 'fuck', 'geico', 'good', 'got', 'great', 'ha', 'harri', 'harvard', 'hate', 'hi', 'hilton', 'honda', 'imposs', 'joli', 'just', 'know', 'laker', 'left', 'like', 'littl', 'london', 'look', 'lot', 'love', 'm', 'macbook', 'make', 'miss', 'mission', 'mit', 'mountain', 'movi', 'need', 'new', 'oh', 'onli', 'pari', 'peopl', 'person', 'potter', 'purdu', 'realli', 'right', 'rock', 's', 'said', 'san', 'say', 'seattl', 'shanghai', 'stori', 'stupid', 'suck', 't', 'thi', 'thing', 'think', 'time', 'tom', 'toyota', 'ucla', 've', 'vinci', 'wa', 'want', 'way', 'whi', 'work']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the counts of each words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179 aaa\n",
      "485 amaz\n",
      "1765 angelina\n",
      "3170 awesom\n",
      "2146 beauti\n",
      "1694 becaus\n",
      "2190 boston\n",
      "2000 brokeback\n",
      "423 citi\n",
      "2003 code\n",
      "481 cool\n",
      "2031 cruis\n",
      "439 d\n",
      "2087 da\n",
      "433 drive\n",
      "1926 francisco\n",
      "477 friend\n",
      "452 fuck\n",
      "1085 geico\n",
      "773 good\n",
      "571 got\n",
      "1178 great\n",
      "776 ha\n",
      "2094 harri\n",
      "2103 harvard\n",
      "4492 hate\n",
      "794 hi\n",
      "2086 hilton\n",
      "2192 honda\n",
      "1098 imposs\n",
      "1764 joli\n",
      "1054 just\n",
      "896 know\n",
      "2019 laker\n",
      "425 left\n",
      "4080 like\n",
      "507 littl\n",
      "2233 london\n",
      "811 look\n",
      "421 lot\n",
      "10334 love\n",
      "1568 m\n",
      "1059 macbook\n",
      "631 make\n",
      "1098 miss\n",
      "1101 mission\n",
      "1340 mit\n",
      "2081 mountain\n",
      "1207 movi\n",
      "1220 need\n",
      "459 new\n",
      "551 oh\n",
      "674 onli\n",
      "2094 pari\n",
      "1018 peopl\n",
      "454 person\n",
      "2093 potter\n",
      "1167 purdu\n",
      "2126 realli\n",
      "661 right\n",
      "475 rock\n",
      "3914 s\n",
      "495 said\n",
      "2038 san\n",
      "627 say\n",
      "2019 seattl\n",
      "1189 shanghai\n",
      "467 stori\n",
      "2886 stupid\n",
      "4614 suck\n",
      "1455 t\n",
      "1705 thi\n",
      "662 thing\n",
      "1524 think\n",
      "781 time\n",
      "2117 tom\n",
      "2028 toyota\n",
      "2008 ucla\n",
      "774 ve\n",
      "2001 vinci\n",
      "3703 wa\n",
      "1656 want\n",
      "932 way\n",
      "547 whi\n",
      "512 work\n"
     ]
    }
   ],
   "source": [
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bag-of-words linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus_data_features_nd[:len(train_data_df)],\n",
    "                                                    train_data_df.Sentiment,\n",
    "                                                    train_size=0.85,\n",
    "                                                    random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       467\n",
      "          1       0.99      0.98      0.99       596\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-train our model with all training data and use if for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 I love MIT so much...\n",
      "0 And I'll never get whad she's saying with her stupid Shanghai accent! >:(\n",
      "1 Previously, I have installed Windoze just so I can show my PC faithful friends how awesome MacBook Pro is.\n",
      "1 I WANT MIT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!..\n",
      "1 i love our trips out to london pissed out of our faces on shitty booze...\n",
      "0 Boston SUCKS.\n",
      "0 Besides, we need at least one ex-boxer in Boston...\n",
      "1 I liked Tom Cruise until he dumped Nicole Kidman.\n",
      "0 i hate the Lakers too but this isn't a basketball blog, so i won't go into it).\n",
      "0 I understand that she'll be taking off to work for Southwest Airlines, which is fantastic too.\n",
      "0 Well apparently all people driving toyota 4runners are appalingly ugly, or so one would think based on him not even glancing in my direction!\n",
      "0 I HATE LONDON!..\n",
      "1 angelina jolie is so beautiful that i don't even have the desire to attain such exquisite beauty..\n",
      "0 we have a boring as shit blue 2005 toyota carolla.\n",
      "1 I'm loving Shanghai > > > ^ _ ^.\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "log_mode = LogisticRegression()\n",
    "log_model.fit(X=corpus_data_features_nd[:len(train_data_df)], y=train_data_df.Sentiment)\n",
    "\n",
    "test_pred = log_model.predict(corpus_data_features_nd[len(train_data_df):])\n",
    "\n",
    "import random\n",
    "spl = random.sample(range(len(test_pred)), 15)\n",
    "\n",
    "for text, sentiment in zip(test_data_df.Text[spl], test_pred[spl]):\n",
    "    print(sentiment, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
