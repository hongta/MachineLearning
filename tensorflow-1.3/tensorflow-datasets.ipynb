{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Datasets is a new way to create input pipelines to TensorFlow models. This API is much more performant than using feed_dict or the queue-based pipelines, and it's cleaner and easier to use. \n",
    "\n",
    "![](datasets.jpg)\n",
    "\n",
    "Where:\n",
    "  - `Dataset`: Base class containing methods to create and transform datasets.   \n",
    "    Also allows you initialize a dataset from data in memory, or from a Python generator.\n",
    "  - `TextLineDataset`: Reads lines from text files.\n",
    "  - `TFRecordDataset`: Reads records from `TFRecord` files.\n",
    "  - `FixedLengthRecordDataset`: Reads fixed size records from binary files.\n",
    "  - `Iterator`: Provides a way to access one dataset element at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_DATASET = \"./dataset\"\n",
    "PATH_MODEL = './model'\n",
    "\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an input reading a file using the Dataset API\n",
    "# Then provide the results to Estimator API\n",
    "\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    \"\"\"\n",
    "    file_path: The data file to read.\n",
    "    perform_shuffle: Whether the record order should be randomized.\n",
    "    repeat_count: The number of times to iterate over the records in the dataset. \n",
    "                For example, if we specify 1, then each record is read once. \n",
    "                If we specify None, iteration will continue forever.\n",
    "    \"\"\"\n",
    "    \n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.],[0.],[0.],[0.],[0]])\n",
    "        label = parsed_line[-1:]\n",
    "        del parsed_line[-1]\n",
    "        features = parsed_line\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "    \n",
    "    dataset = (tf.contrib.data.TextLineDataset(file_path)\n",
    "               .skip(1) # skip header row\n",
    "               .map(decode_csv))\n",
    "    \n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)\n",
    "    dataset = dataset.batch(32)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    \n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the return value of `my_input_fn` must be two-element tuple organized as following:\n",
    "  - `batch_features` must be a dict in which each input feature is a key and then a list of values for the training batch.\n",
    "  - `batch_labels` is a 1-d tensor of labels for the training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch = my_input_fn(FILE_TRAIN, True) # return 32 random elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators is a high-level API that reduces much of the boilerplate code you previously needed to write when training a Tensorflow model. Estimator are also very flexible, allowing you toe override the default behavior if you have specific requestments for your model.  \n",
    "\n",
    "There are two possible ways can build your model using Estimators:\n",
    "  - `Pre-made Estimator`: There are predifined estimators, created to generate a specific type of model.\n",
    "  - `Estimator(base class)` Gives you complete control of how your model should be created by using a `model_fn` function.\n",
    "  \n",
    "![](estimator.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[10, 10], # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 52.255, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 30 into ./model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 12.1572.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1a17c52e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "\n",
    "classifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator request a `input_fn` with no arguments, so we create one with `lambda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-10-14-08:02:02\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-30\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-14-08:02:02\n",
      "INFO:tensorflow:Saving dict for global step 30: accuracy = 0.7, average_loss = 0.448881, global_step = 30, loss = 13.4664\n",
      "Evaluation results\n",
      "   accuracy, was: 0.699999988079071\n",
      "   average_loss, was: 0.44888123869895935\n",
      "   loss, was: 13.466437339782715\n",
      "   global_step, was: 30\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# return value will contain evaluation_metrics such as: loss & average_loss\n",
    "\n",
    "evaluate_result = classifier.evaluate(input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicte trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-30\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Predict the type of some Iris flowers.\n",
    "predict_results = classifier.predict(input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
    "    print(prediction[\"class_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4) # Need to split into 4 features.\n",
    "        return dict(zip(feature_names, x))\n",
    "    \n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None # In prediction, we have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on memory\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-30\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Virginica\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "print(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    _type = prediction[\"class_ids\"][0] # the the predicted class\n",
    "    if _type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif _type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.8882139463355622&quot;).pbtxt = 'node {\\n  name: &quot;filenames&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;./dataset/iris_training.csv&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;compression_type&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;filenames_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;./dataset/iris_training.csv&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;compression_type_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;buffer_size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 256\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;seed&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;seed2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;OneShotIterator&quot;\\n  op: &quot;OneShotIterator&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dataset_factory&quot;\\n    value {\\n      func {\\n        name: &quot;_make_dataset_82ab5ea5&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_shapes&quot;\\n    value {\\n      list {\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_types&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_INT32\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;IteratorGetNext&quot;\\n  op: &quot;IteratorGetNext&quot;\\n  input: &quot;OneShotIterator&quot;\\n  attr {\\n    key: &quot;output_shapes&quot;\\n    value {\\n      list {\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_types&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_INT32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;filenames_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;./dataset/iris_test.csv&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;compression_type_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count_3&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;count_4&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n        }\\n        int64_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;OneShotIterator_1&quot;\\n  op: &quot;OneShotIterator&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dataset_factory&quot;\\n    value {\\n      func {\\n        name: &quot;_make_dataset_ef5737b5&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_shapes&quot;\\n    value {\\n      list {\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_types&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_INT32\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;IteratorGetNext_1&quot;\\n  op: &quot;IteratorGetNext&quot;\\n  input: &quot;OneShotIterator_1&quot;\\n  attr {\\n    key: &quot;output_shapes&quot;\\n    value {\\n      list {\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n        }\\n        shape {\\n          dim {\\n            size: -1\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;output_types&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_INT32\\n      }\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.8882139463355622&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
