{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [How to Prepare Text Data for Deep Learning with Keras](https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is divided into 4 parts:\n",
    "  - Split words with `text_to_word_sequence`.\n",
    "  - Encoding with `one_hot`\n",
    "  - hash Encoding with `hasing_trick`\n",
    "  - Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Words with `text_to_word_sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this function automatically does 3 things:\n",
    "  - Splits words by space(split=\" \")\n",
    "  - Filters out punctuation(filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’)\n",
    "  - Convert text to lowercase(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', \"tim's\"]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog. tim's\"\n",
    "r = text_to_word_sequence(text)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding with `one_hot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the `one_hot()` function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one-hot encoding of the document, which is not the case.   \n",
    "Instead, the function is a wrapper for the `hashing_trick()` function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9\n",
      "[2, 2, 6, 5, 8, 3, 2, 1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog. tim's\"\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "r = one_hot(text, round(vocab_size*1.3))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Encoding with `hashing_trick`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.\n",
    "\n",
    "An alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n",
    "\n",
    "Keras provides the `hashing_trick()` function that tokenizes and then integer encodes the document, just like the `one_hot()` function. It provides more flexibility, allowing you to specify the hash function as either ‘hash’ (the default) or other hash functions such as the built in md5 function or your own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[7, 2, 8, 11, 7, 7, 7, 8, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog. tim's\"\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "r = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n",
    "\n",
    "Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# there are 5 documents\n",
    "docs = ['Well done!',\n",
    "    'Good work',\n",
    "    'Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!']\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - word_counts: A dictionary of words and their counts.\n",
    "  - word_docs: An integer count of the total number of documents that were used to fit the Tokenizer.\n",
    "  - word_index: A dictionary of words and their uniquely assigned integers.\n",
    "  - document_count: A dictionary of words and how many documents each appeared in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1}\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets.\n",
    "\n",
    "The `texts_to_matrix()` function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary.\n",
    "\n",
    "This function provides a suite of standard `bag-of-words` model text encoding schemes that can be provided via a mode argument to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modes avaiable include:\n",
    "  - 'binary': Whether or not each word is present in the document.This is the default.\n",
    "  - 'count': The count of each word in the document\n",
    "  - 'tfidf': The Text Frequency-Inverse DocumentFrequency(TF-IDF) scoring for each word\n",
    "  - 'freq': The frequency of each word as a ratio of words within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1}\n",
      "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# there are 5 documents\n",
    "docs = ['Well done!',\n",
    "    'Good work',\n",
    "    'Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!']\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode=\"count\")\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
