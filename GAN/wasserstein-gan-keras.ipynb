{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: [Wassertein ACGAN with Keras](https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras import layers, models, optimizers, callbacks, initializers\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: 2.0.8\n",
      "Tensorflow: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras:\", keras.__version__)\n",
    "print('Tensorflow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "RND = 777\n",
    "\n",
    "# output settings\n",
    "RUN = 'F'\n",
    "OUT_DIR = 'out/' + RUN\n",
    "TENSORBOARD_DIR = 'model/' + RUN\n",
    "SAVE_SAMPLE_IMAGES = False\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "ITERATIONS = 20000\n",
    "\n",
    "# size of the random vector used to initialize G\n",
    "Z_SIZE = 100\n",
    "\n",
    "# number of iterations D is trained for per each G iteration\n",
    "D_ITERS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(OUT_DIR): os.makedirs(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basically return mean(y_pred)\n",
    "# but with ability to inverse it for minimization (when y_true == -1)\n",
    "def wasserstein(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_D():\n",
    "    \n",
    "    # weights are initialized from normal distribution with below params\n",
    "    weight_init = initializers.RandomNormal(mean=0., stddev=0.02)\n",
    "    \n",
    "    input_image = layers.Input(shape=(28, 28, 1), name='input_image')\n",
    "    \n",
    "    x = layers.Conv2D(32,\n",
    "                      (3, 3),\n",
    "                      padding='same',\n",
    "                      name='conv_1',\n",
    "                      kernel_initializer=weight_init)(input_image)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64,\n",
    "                      (3, 3),\n",
    "                      padding='same',\n",
    "                      name='conv_2',\n",
    "                      kernel_initializer=weight_init)(x)\n",
    "    x = layers.MaxPool2D(pool_size=1)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128,\n",
    "                      (3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv_3',\n",
    "                      kernel_initializer=weight_init)(x)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256,\n",
    "                      (3, 3),\n",
    "                      padding='same',\n",
    "                      name='conv_4',\n",
    "                      kernel_initializer=weight_init)(x)\n",
    "    x = layers.MaxPool2D(pool_size=1)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    features = layers.Flatten()(x)\n",
    "    \n",
    "    output_is_fake = layers.Dense(1,\n",
    "                                  activation='linear',\n",
    "                                  name='output_is_fake')(features)\n",
    "    output_class = layers.Dense(10,\n",
    "                                activation='softmax',\n",
    "                                name='output_class')(features)\n",
    "    return models.Model(inputs=[input_image],\n",
    "                        outputs=[output_is_fake, output_class], name='D')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_G(Z_SIZE=Z_SIZE):\n",
    "    DICT_LEN = 10\n",
    "    EMBEDDING_LEN = Z_SIZE\n",
    "    \n",
    "    weight_init = initializers.RandomNormal(mean=0., stddev=0.02)\n",
    "    \n",
    "    input_class = layers.Input(shape=(1, ), dtype='int32', name='input_class')\n",
    "    \n",
    "    e = layers.Embedding(DICT_LEN,\n",
    "                         EMBEDDING_LEN,\n",
    "                         embeddings_initializer='glorot_uniform')(input_class)\n",
    "    embedded_class = layers.Flatten(name='embedded_class')(e)\n",
    "    \n",
    "    input_z = layers.Input(shape=(Z_SIZE, ), name='input_z')\n",
    "    \n",
    "    # hadamard product\n",
    "    h = layers.multiply([input_z, embedded_class], name='h')\n",
    "    \n",
    "    # cnn part\n",
    "    x = layers.Dense(1024)(h)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(128 * 7 * 7)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((7, 7, 128))(x)\n",
    "    \n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = layers.Conv2D(256, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = layers.Conv2D(128, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(1,\n",
    "                      (2, 2),\n",
    "                      padding='same',\n",
    "                      activation='tanh', \n",
    "                      name='output_generated_image',\n",
    "                      kernel_initializer=weight_init)(x)\n",
    "    return models.Model(inputs=[input_z, input_class], outputs=x, name='G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = create_D()\n",
    "\n",
    "D.compile(optimizer=optimizers.RMSprop(lr=5e-4),\n",
    "          loss=[wasserstein, 'sparse_categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_z = layers.Input(shape=(Z_SIZE,), name='input_z_')\n",
    "input_class = layers.Input(shape=(1, ), name='input_class_', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = create_G()\n",
    "\n",
    "# create combined D(G) model\n",
    "output_is_fake, output_class = D(G(inputs=[input_z, input_class]))\n",
    "\n",
    "DG = models.Model(inputs=[input_z, input_class], outputs=[output_is_fake, output_class])\n",
    "DG.get_layer('D').trainable = False\n",
    "\n",
    "DG.compile(optimizer=optimizers.RMSprop(lr=5e-5),\n",
    "           loss=[wasserstein, 'sparse_categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# use all avaiable 70k samples\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "y_train = np.concatenate((y_train, y_test))\n",
    "\n",
    "# convert to -1..1 range, reshape to (sample, 28, 28, 1)\n",
    "X_train = (X_train.astype('float32') - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_zz = np.random.normal(0., 1., (100, Z_SIZE))\n",
    "\n",
    "def generate_samples(n=0, save=True):\n",
    "    \n",
    "    generated_classes = np.array(list(range(0, 10)) * 10)\n",
    "    #print(generated_classes.shape)\n",
    "    generated_images = G.predict([samples_zz, generated_classes.reshape(-1, 1)])\n",
    "    \n",
    "    rr = []\n",
    "    for c in range(10):\n",
    "        rr.append(np.concatenate(generated_images[c * 10: (1+c) * 10]).reshape(280, 28))\n",
    "    img = np.hstack(rr)\n",
    "    \n",
    "    if save:\n",
    "        plt.imsave(OUT_DIR + '/samples_%7d.png' %n, img, cmap=plt.cm.gray)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to tensorboard summaries\n",
    "sw = tf.summary.FileWriter(TENSORBOARD_DIR)\n",
    "def update_tb_summary(step, sample_images=True, save_image_files=True):\n",
    "    \n",
    "    s = tf.Summary()\n",
    "    \n",
    "    # losses as is\n",
    "    for names, vals in zip((('D_real_is_fake', 'D_real_class'),\n",
    "                            ('D_fake_is_fake', 'D_fake_class'),\n",
    "                            ('DG_is_fake', 'DG_class')),\n",
    "                           (D_true_losses, D_fake_losses, DG_losses)):\n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][1]\n",
    "        v.tag = names[0]\n",
    "        \n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][2]\n",
    "        v.tag = names[1]\n",
    "        \n",
    "    # D loss: -1 * D_true_is_fake - D_fake_is_fake\n",
    "    v = s.value.add()\n",
    "    v.simple_value = -D_true_losses[-1][1] - D_fake_losses[-1][1]\n",
    "    v.tag = 'D loss (-1*D_real_is_fake - D_fake_is_fake)'\n",
    "    \n",
    "    # generated image\n",
    "    if sample_images:\n",
    "        img = generate_samples(step, save=save_image_files)\n",
    "        s.MergeFromString(tf.Session().run(\n",
    "            tf.summary.image('sample_%07d' % step,\n",
    "                             img.reshape([1, *img.shape, 1]))))\n",
    "    \n",
    "    sw.add_summary(s, step)\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6001/20000 [========>.....................] - ETA: 14090s - D_real_is_fake: -1.8342 - D_real_class: 0.8833 - D_fake_is_fake: -0.1810 - D_fake_class: 1.3467 - D(G)_is_fake: 2.4783 - D(G)_class: 1.4183"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4783195158fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mzz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mgenerated_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fake = 1\n",
    "# real = -1\n",
    "\n",
    "progress_bar = Progbar(target=ITERATIONS)\n",
    "\n",
    "DG_losses = []\n",
    "D_true_losses = []\n",
    "D_fake_losses = []\n",
    "\n",
    "for it in range(ITERATIONS):\n",
    "    \n",
    "    if len(D_true_losses) > 0:\n",
    "        progress_bar.update(\n",
    "            it,\n",
    "            values=[('D_real_is_fake', np.mean(D_true_losses[-5:], axis=0)[1]),\n",
    "                    ('D_real_class', np.mean(D_true_losses[-5:], axis=0)[2]),\n",
    "                    ('D_fake_is_fake', np.mean(D_fake_losses[-5:], axis=0)[1]),\n",
    "                    ('D_fake_class', np.mean(D_fake_losses[-5:], axis=0)[2]),\n",
    "                    ('D(G)_is_fake', np.mean(DG_losses[-5:], axis=0)[1]),\n",
    "                    ('D(G)_class', np.mean(DG_losses[-5:], axis=0)[2])\n",
    "                   ]\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        progress_bar.update(it)\n",
    "        \n",
    "    # 1: train D on real+generated images\n",
    "    \n",
    "    if (it % 1000) < 25 or it % 500 == 0: # 25 times in 1000, every 500th\n",
    "        d_iters = 100\n",
    "    else:\n",
    "        d_iters = D_ITERS\n",
    "    \n",
    "    for d_it in range(d_iters):\n",
    "        \n",
    "        # unfreeze D\n",
    "        for l in D.layers:\n",
    "            l.trainable = True\n",
    "        \n",
    "        for l in D.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "            \n",
    "        # 1.1 maximize D output on reals == minimize -1 * (D(real))\n",
    "        \n",
    "        # draw random samples from real images\n",
    "        index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "        real_images = X_train[index]\n",
    "        real_images_classes = y_train[index]\n",
    "        \n",
    "        D_loss = D.train_on_batch(real_images, [-np.ones(BATCH_SIZE), real_images_classes])\n",
    "        D_true_losses.append(D_loss)\n",
    "        \n",
    "        # 1.2 minimize D output on fakes\n",
    "        zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE))\n",
    "        generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "        generated_images = G.predict([zz, generated_classes.reshape(-1, 1)])\n",
    "        \n",
    "        D_loss = D.train_on_batch(generated_images, [np.ones(BATCH_SIZE), generated_classes])\n",
    "        D_fake_losses.append(D_loss)\n",
    "        \n",
    "    # 2: train D(G) (D is frozen)\n",
    "    # minimize D output whle supplying it with fakes, telling it that they are reals(-1)\n",
    "    D.trainable = False\n",
    "    for l in D.layers: \n",
    "        l.trainable = False\n",
    "    \n",
    "    zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE))\n",
    "    generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "    \n",
    "    DG_loss = DG.train_on_batch(\n",
    "        [zz, generated_classes.reshape((-1, 1))],\n",
    "        [-np.ones(BATCH_SIZE), generated_classes])\n",
    "    \n",
    "    DG_losses.append(DG_loss)\n",
    "    \n",
    "    if it % 10 == 0:\n",
    "        update_tb_summary(it, sample_images=(it % 10 == 0), save_image_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
